{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f172ea05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import emot\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import csv\n",
    "from sklearn.cluster import KMeans\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbae194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary of terms with valence, arousal, and dominance rankings\n",
    "\n",
    "file_name = '/joy/joyData/WordRankings/NRC-VAD-Lexicon.txt'\n",
    "\n",
    "terms = {}\n",
    "va = []\n",
    "ar = []\n",
    "do = []\n",
    "counter=0 \n",
    "\n",
    "# read words file and store valence, arousal, and dominance in a dictionary\n",
    "with open(file_name, 'r') as f:\n",
    "    lines=f.readlines()\n",
    "    \n",
    "    for row in lines:\n",
    "        row_=row.split(\"\\t\")\n",
    "        try:\n",
    "            terms[row_[0]] = counter\n",
    "            va.append(float(row_[1]))\n",
    "            ar.append(float(row_[2]))\n",
    "            do.append(float(row_[3]))\n",
    "            counter +=1\n",
    "        except:\n",
    "            print(row_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb39e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorization\n",
    "df = pd.read_csv('/joy/joyData/allData')\n",
    "\n",
    "count_vectorizer = CountVectorizer(vocabulary=terms, ngram_range=(1,2))\n",
    "count_vectors = count_vectorizer.fit_transform(df['clean_tweet'].values.astype('U')).toarray()\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(vocabulary=terms, ngram_range=(1,2))\n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform(df['clean_tweet'].values.astype('U')).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c300e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to remove unnecessary data\n",
    "\n",
    "def remove_zeros(vectors):\n",
    "    \n",
    "    #print the shape of the original matrix\n",
    "    print(\"original matrix size\", vectors.shape)\n",
    "\n",
    "    #mark all-zero columns\n",
    "    mask = (vectors == 0).all(0)\n",
    "\n",
    "    # Find the indices of these columns (we might need those)\n",
    "    column_indices = np.where(mask)[0]\n",
    "\n",
    "    # Update vectors to only include the columns where non-zero values occur.\n",
    "    vectors = vectors[:,~mask]\n",
    "\n",
    "    #print the shape of the new vector matrix\n",
    "    print(\"compact matrix size\", vectors.shape)\n",
    "    \n",
    "    return vectors\n",
    "\n",
    "def remove_sparse_rows(vectors, n_words):\n",
    "    return vectors[np.count_nonzero(vectors==0, axis=1) < vectors.shape[1]-n_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1821b8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLUSTERING FUNCTIONS\n",
    "\n",
    "# selecting best value for k\n",
    "def best_k(vectors):\n",
    "    \n",
    "    Sum_of_squared_distances = []\n",
    "    k = range(4,15)\n",
    "    for num_clusters in k :\n",
    "        kmeans = KMeans(n_clusters=num_clusters, max_iter = 1000)\n",
    "        kmeans.fit(vectors)\n",
    "        Sum_of_squared_distances.append(kmeans.inertia_)\n",
    " \n",
    "    plt.plot(k,Sum_of_squared_distances,'bx-')\n",
    "    plt.xlabel('Values of K') \n",
    "    plt.ylabel('Sum of squared distances/Inertia') \n",
    "    plt.title('Best Number of Clusters')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "# clustering\n",
    "def clust(vectors, k):\n",
    "    model = KMeans(n_clusters=k, init='k-means++', max_iter=1000, n_init=10)\n",
    "    kmeans_vectors = model.fit_predict(vectors)\n",
    "\n",
    "\n",
    "    #let's plot the PCA \n",
    "    pca = PCA(2)\n",
    "    pca.fit(vectors)\n",
    "    docs_pca = pca.transform(vectors)\n",
    "\n",
    "\n",
    "    plt.figure(figsize = (8, 8))\n",
    "    plt.grid()\n",
    "    sns.scatterplot(x=docs_pca[:, 0], y=docs_pca[:, 1], hue=kmeans_vectors, palette=sns.color_palette(\"bright\", k))\n",
    "    plt.show()\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# create word clouds\n",
    "def word_clouds(base_vectors, clust_model, k):\n",
    "\n",
    "    k_ = clust_model.predict(base_vectors)\n",
    "\n",
    "    result={'cluster':k_,'data':df['clean_tweet']}\n",
    "    result=pd.DataFrame(result)\n",
    "\n",
    "\n",
    "    stopWords = stopwords.words('english')\n",
    "    stopWords.extend(['joy', 'amp', 'happy', 'people', 'love', 'chicago', 'time', 'much', 'thank', 'bring', 'day'])\n",
    "\n",
    "    for k in range(0,k):\n",
    "       print(\"Cluster {}\\n\".format(k))\n",
    "       s=result[result.cluster==k]\n",
    "       text=s['data'].str.cat(sep=' ')\n",
    "       text=' '.join([word for word in text.split() if word.lower() not in stopWords])\n",
    "       wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text)\n",
    "  \n",
    "       plt.figure()\n",
    "       plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "       plt.axis(\"off\")\n",
    "       plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f78c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF VECTORS CLUSTERING COMPARISON\n",
    "a = remove_sparse_rows(tfidf_vectors, 1)\n",
    "\n",
    "tfidf_vectors.shape, a.shape\n",
    "\n",
    "# selecting best value for k\n",
    "best_k(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b543f14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering on tfidf vectors with sparse rows with only 1 word removed\n",
    "a_model = clust(a, k=13)\n",
    "word_clouds(tfidf_vectors, a_model, k=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4d1b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = remove_sparse_rows(tfidf_vectors, 2)\n",
    "\n",
    "tfidf_vectors.shape, b.shape\n",
    "\n",
    "# selecting best value for k\n",
    "best_k(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ccf62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sum_of_squared_distances = []\n",
    "k = range(15,20)\n",
    "for num_clusters in k :\n",
    "    kmeans = KMeans(n_clusters=num_clusters, max_iter = 1000)\n",
    "    kmeans.fit(b)\n",
    "    Sum_of_squared_distances.append(kmeans.inertia_)\n",
    " \n",
    "plt.plot(k,Sum_of_squared_distances,'bx-')\n",
    "plt.xlabel('Values of K') \n",
    "plt.ylabel('Sum of squared distances/Inertia') \n",
    "plt.title('Best Number of Clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa09b183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering on tfidf vectors with sparse rows with only 2 words removed\n",
    "b_model = clust(b, k=14)\n",
    "word_clouds(tfidf_vectors, b_model, k=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b97a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = remove_sparse_rows(tfidf_vectors, 1)\n",
    "\n",
    "tfidf_vectors.shape, c.shape\n",
    "\n",
    "# selecting best value for k\n",
    "best_k(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e677d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sum_of_squared_distances = []\n",
    "k = range(15,20)\n",
    "for num_clusters in k :\n",
    "    kmeans = KMeans(n_clusters=num_clusters, max_iter = 1000)\n",
    "    kmeans.fit(c)\n",
    "    Sum_of_squared_distances.append(kmeans.inertia_)\n",
    " \n",
    "plt.plot(k,Sum_of_squared_distances,'bx-')\n",
    "plt.xlabel('Values of K') \n",
    "plt.ylabel('Sum of squared distances/Inertia') \n",
    "plt.title('Best Number of Clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a42070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering on tfidf vectors with sparse rows with only 3 words removed\n",
    "c_model = clust(c, k=19)\n",
    "word_clouds(tfidf_vectors, c_model, k=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d70247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COUNT VECTORS CLUSTERING COMPARISON\n",
    "x = remove_sparse_rows(count_vectors, 1)\n",
    "\n",
    "count_vectors.shape, x.shape\n",
    "\n",
    "# selecting best value for k\n",
    "best_k(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
