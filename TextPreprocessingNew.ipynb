{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f6e405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import RegexpStemmer\n",
    "import emot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bedf2cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for cleaning and stemming tweets for a given month and recording emoji meanings\n",
    "# parameters for removing URLs, hashtags, punctuation, stop words, and converting to lowercase\n",
    "\n",
    "def text_cleaning(month, year, joy, rm_urls, rm_tags, rm_punc, rm_stop, lower):\n",
    "    \n",
    "    # determine if looking at joy or no joy tweets\n",
    "    if joy == True: \n",
    "        path = 'Joy-NoJoy/Joy-'\n",
    "    \n",
    "    else:\n",
    "        path = 'Joy-NoJoy/NoJoy-'\n",
    "        \n",
    "        \n",
    "    file = '/joy/joyData/' + path + str(month) + '-' + str(year) + '.csv'\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    \n",
    "    clean_tweets = []\n",
    "    clean_stems = []\n",
    "    emoji_meanings = []\n",
    "    \n",
    "    for tweet in df['content']:\n",
    "        \n",
    "\n",
    "        # remove URLs\n",
    "        if rm_urls == True:\n",
    "        \n",
    "            url_regex = r\"\\b((?:https?://)?(?:(?:www\\.)?(?:[\\da-z\\.-]+)\\.(?:[a-z]{2,6})|(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)|(?:(?:[0-9a-fA-F]{1,4}:){7,7}[0-9a-fA-F]{1,4}|(?:[0-9a-fA-F]{1,4}:){1,7}:|(?:[0-9a-fA-F]{1,4}:){1,6}:[0-9a-fA-F]{1,4}|(?:[0-9a-fA-F]{1,4}:){1,5}(?::[0-9a-fA-F]{1,4}){1,2}|(?:[0-9a-fA-F]{1,4}:){1,4}(?::[0-9a-fA-F]{1,4}){1,3}|(?:[0-9a-fA-F]{1,4}:){1,3}(?::[0-9a-fA-F]{1,4}){1,4}|(?:[0-9a-fA-F]{1,4}:){1,2}(?::[0-9a-fA-F]{1,4}){1,5}|[0-9a-fA-F]{1,4}:(?:(?::[0-9a-fA-F]{1,4}){1,6})|:(?:(?::[0-9a-fA-F]{1,4}){1,7}|:)|fe80:(?::[0-9a-fA-F]{0,4}){0,4}%[0-9a-zA-Z]{1,}|::(?:ffff(?::0{1,4}){0,1}:){0,1}(?:(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])|(?:[0-9a-fA-F]{1,4}:){1,4}:(?:(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])))(?::[0-9]{1,4}|[1-5][0-9]{4}|6[0-4][0-9]{3}|65[0-4][0-9]{2}|655[0-2][0-9]|6553[0-5])?(?:/[\\w\\.-]*)*/?)\\b\"\n",
    "            tweet = re.sub(url_regex, '', tweet)\n",
    "        \n",
    "        # remove hashtags\n",
    "        if rm_tags == True:\n",
    "            \n",
    "            tag_regex = '/#\\w+\\s*/'\n",
    "            tweet = re.sub(tag_regex, '', tweet)\n",
    "            \n",
    "        # remove punctuation\n",
    "        if rm_punc == True:\n",
    "            \n",
    "            tweet = \"\".join([char for char in tweet if char not in string.punctuation and char not in ['\\n']])\n",
    "        \n",
    "        # convert to lower case\n",
    "        if lower == True:\n",
    "            \n",
    "            tweet = tweet.lower()\n",
    "            \n",
    "        # tokenization (second one removes emojis)\n",
    "        word_tokens = nltk.word_tokenize(tweet)\n",
    "        #word_tokens = RegexpTokenizer(r'\\w+').tokenize(tweet)\n",
    "        \n",
    "        # remove stop words\n",
    "        if rm_stop == True:\n",
    "            \n",
    "            #nltk.download('stopwords')\n",
    "            \n",
    "            non_stopwords = [word for word in word_tokens if word not in stopwords.words('english')]\n",
    "            tweet = \" \".join(non_stopwords)\n",
    "        \n",
    "        clean_tweets.append(tweet)\n",
    "          \n",
    "            \n",
    "        # stemming\n",
    "        regexp = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
    "            \n",
    "        stems = []\n",
    "        for word in word_tokens:\n",
    "            stems.append(regexp.stem(word))\n",
    "                             \n",
    "        stem = \" \".join(stems)\n",
    "    \n",
    "        clean_stems.append(stem)\n",
    "        \n",
    "        \n",
    "        # find emoji meanings\n",
    "        emot_obj = emot.core.emot() \n",
    "        emojis = emot_obj.emoji(tweet)\n",
    "        tweet_emojis = []\n",
    "    \n",
    "        # if there are emojis in the tweet keep the emoji meanings in a list\n",
    "        if len(emojis['value']) > 0:\n",
    "                \n",
    "            for i in range(len(emojis['value'])):\n",
    "                    \n",
    "                meaning = emojis['mean'][i]\n",
    "                clean_meaning = meaning.replace(\"_\", \" \")\n",
    "                clean_meaning = meaning.replace(\":\", \"\")\n",
    "                tweet_emojis.append(clean_meaning)\n",
    "        \n",
    "        emoji_meanings.append(tweet_emojis)\n",
    "                            \n",
    "        \n",
    "        \n",
    "    # write df to file with new column that includes the cleaned tweets, stems, and emoji meanings\n",
    "    df['clean_tweet'] = clean_tweets\n",
    "    df['clean_stems'] = clean_stems\n",
    "    df['emoji_meanings'] = emoji_meanings\n",
    "    df.to_csv(path + str(month) + '-' + str(year) + '.csv', index = False, mode = 'w')\n",
    "    \n",
    "    return clean_tweets\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75af6f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['09', '2019']\n",
      "['10', '2019']\n",
      "['11', '2019']\n",
      "['12', '2019']\n",
      "['01', '2020']\n",
      "['02', '2020']\n",
      "['03', '2020']\n",
      "['04', '2020']\n",
      "['05', '2020']\n",
      "['06', '2020']\n",
      "['07', '2020']\n",
      "['08', '2020']\n",
      "['09', '2020']\n",
      "['10', '2020']\n",
      "['11', '2020']\n",
      "['12', '2020']\n",
      "['01', '2021']\n",
      "['02', '2021']\n",
      "['03', '2021']\n",
      "['04', '2021']\n",
      "['05', '2021']\n",
      "['06', '2021']\n",
      "['07', '2021']\n",
      "['08', '2021']\n",
      "['09', '2021']\n",
      "['10', '2021']\n",
      "['11', '2021']\n",
      "['12', '2021']\n",
      "['01', '2022']\n"
     ]
    }
   ],
   "source": [
    "# create a list of months to loop through for counting joy tweets \n",
    "dates = pd.date_range(start = '09/01/2019', end = '01/31/2022', freq = 'M')\n",
    "\n",
    "\n",
    "months = []\n",
    "for date in dates:\n",
    "    date = str(date).split('-')\n",
    "    year = date[0]\n",
    "    month = date[1]\n",
    "    months.append([month, year])\n",
    "    \n",
    "    \n",
    "# loop through months and clean both joy and no joy tweets\n",
    "\n",
    "for month in months:\n",
    "    \n",
    "    text_cleaning(month = month[0], year = month[1], joy = True, rm_urls = True, rm_tags = False, rm_punc = True, rm_stop = True, lower = True)\n",
    "    text_cleaning(month = month[0], year = month[1], joy = False, rm_urls = True, rm_tags = False, rm_punc = True, rm_stop = True, lower = True)\n",
    "    print(month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61ea0724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Word Porter Lancaster  RegExp Snowball   Lemma\n",
      "0     joy    joy       joy     joy      joy     joy\n",
      "1    joys    joy       joy     joy      joy     joy\n",
      "2   enjoy  enjoy     enjoy   enjoy    enjoy   enjoy\n",
      "3  joyful    joy       joy  joyful      joy  joyful\n",
      "4  joyous  joyou       joy   joyou   joyous  joyous\n"
     ]
    }
   ],
   "source": [
    "# Determining the Best Stemmer/Lemmatizer for above function \n",
    "\n",
    "# which ones convert words like \"enjoy\", \"joyous\", or \"joyful\" to \"joy\"?\n",
    "\n",
    "\n",
    "import nltk \n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import RegexpStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "\n",
    "# create an object for each of the stemmers\n",
    "porter = PorterStemmer()\n",
    "lancaster=LancasterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "regexp = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
    "snowball = SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "words = ['joy', 'joys', 'enjoy', 'joyful', 'joyous']\n",
    "\n",
    "stems = []\n",
    "for word in words:\n",
    "  stems.append([word, porter.stem(word), lancaster.stem(word), regexp.stem(word), snowball.stem(word), wordnet_lemmatizer.lemmatize(word, pos='v')])\n",
    "\n",
    "\n",
    "df = pd.DataFrame(stems, columns=['Word', 'Porter', 'Lancaster', 'RegExp', 'Snowball', 'Lemma'])\n",
    "print(df)\n",
    "\n",
    "# Lancaster includes the most variations of joy\n",
    "# Using regular expressions to stem or word net lemmatizer includes the least\n",
    "# Snowball and Porter include joy and joyful but not joyous\n",
    "# None include enjoy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
